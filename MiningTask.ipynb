{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mining Task**\n",
        "### By Mohammadreza Saeidi\n",
        "\n",
        "Using Jira, this project collects all Bug Reports (BR) submitted. It then extracts all commits that add lines to the source code (not modify or delete lines). Finally, it determines corresponding Bug Fixing Changes for each BR.\n",
        "\n",
        "Some tips tht are used in this project are inspired by Borg et al.'s paper [1]. e.g., Regex and JQL for Jira.\n",
        "\n",
        "## Reference:\n",
        "> [1] M. Borg, O. Svensson, K. Berg, and D. Hansson, “SZZ Unleashed: An Open Implementation of the SZZ Algorithm - Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the Jenkins Project,” in *Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation*, 2019, pp. 7–12. [doi:10.1145/3340482.3342742](https://arxiv.org/abs/1903.01742)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GKdfxagBp1SI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUzjegkYII1Z"
      },
      "outputs": [],
      "source": [
        "# Using PyDriller library to retrieve commits\n",
        "!pip install pydriller\n",
        "\n",
        "# Using Jira library to retrieve Issues\n",
        "!pip install jira"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurations\n",
        "GIT_URL = \"https://github.com/apache/activemq.git\"\n",
        "GIT_BRANCH_NAME = \"main\"\n",
        "JIRA_URL = \"https://issues.apache.org/jira/\"\n",
        "JIRA_MAX_RESULT_COUNT = 1000\n",
        "PROJECT_NAME = \"ActiveMQ\"\n",
        "PROJECT_BRIEF_NAME = \"AMQ\"\n",
        "#Obtained from [1]\n",
        "ISSUE_TYPES = [\"Bug\"]\n",
        "ISSUE_STATUS = [\"Resolved\", \"Closed\"]\n",
        "ISSUE_RESOLUTION = [\"Fixed\"]"
      ],
      "metadata": {
        "id": "kIdn7bgis20K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dateutil.parser\n",
        "from datetime import datetime\n",
        "import re"
      ],
      "metadata": {
        "id": "Q5lNUAOG47TC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving Bug Reports from Jira"
      ],
      "metadata": {
        "id": "uzu4_iZiwsSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jira import JIRA\n",
        "def fetch_issues(jira_server_url:str, jql:str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  It takes the URL of a Jira server and a query to retrieve and \n",
        "  return all issues matching the given query in pd.DataFrame.\n",
        "  \"\"\"\n",
        "  fetched_issues = pd.DataFrame(columns=[\n",
        "      'issue_id', \n",
        "      'issue_key', \n",
        "      'issue_created_date', \n",
        "      'issue_summary', \n",
        "      'issue_description'\n",
        "      ])\n",
        "  \n",
        "  last_fetched_index = 0\n",
        "  \n",
        "  jira_server = JIRA(server = jira_server_url)\n",
        "  \n",
        "  # Retrieve Issues iteratively and JIRA_MAX_RESULT_COUNT (default = 1000) issues in each iteration\n",
        "  while True:\n",
        "    print(f\"Fetching Issues: iteraation #{(last_fetched_index//JIRA_MAX_RESULT_COUNT) + 1}\")\n",
        "\n",
        "    result = jira_server.search_issues(\n",
        "        jql_str = jql, \n",
        "        fields = ['created', 'summary', 'description'], \n",
        "        startAt = last_fetched_index, \n",
        "        maxResults = JIRA_MAX_RESULT_COUNT\n",
        "        )\n",
        "    \n",
        "    for issue in result:\n",
        "      new_row = {\n",
        "          'issue_id' : issue.id, \n",
        "          'issue_key' : issue.key, \n",
        "          'issue_created_date': dateutil.parser.isoparse(issue.get_field('created')), \n",
        "          'issue_summary' : issue.get_field('summary'), \n",
        "          'issue_description': issue.get_field('description')\n",
        "          }\n",
        "      fetched_issues = fetched_issues.append(new_row, ignore_index = True)\n",
        "\n",
        "    print(f\"{len(result)} new Issues Recieved, All fetched issues: {len(fetched_issues)} out of {result.total}\")\n",
        "\n",
        "    # Breaking the loop when all issues have been retrieved\n",
        "    if fetched_issues.shape[0] == result.total:\n",
        "      print(\"All issues have been recieved.\")\n",
        "      break\n",
        "\n",
        "    last_fetched_index += len(result)\n",
        "\n",
        "  return fetched_issues\n",
        "\n",
        "\n",
        "def create_jql(project_name:str, issue_types:list, issue_status:list, issue_resolution:list) -> str:\n",
        "  \"\"\"\n",
        "  Given project name, issue types, issue status, and issue resolution, \n",
        "  this method generates a Query that can be used for retrieving issues from Jira.\n",
        "  \"\"\"\n",
        "  jql = \"\"\n",
        "\n",
        "  if project_name:\n",
        "    jql += f\"project = {project_name}\"\n",
        "  \n",
        "  if issue_types:\n",
        "    if jql:\n",
        "      jql += \" AND \"\n",
        "    jql += f\"issuetype in ({','.join(issue_types)})\"\n",
        "\n",
        "  if issue_status:\n",
        "    if jql:\n",
        "      jql += \" AND \"\n",
        "    jql += f\"status in ({','.join(issue_status)})\"\n",
        "\n",
        "  if issue_resolution:\n",
        "    if jql:\n",
        "      jql += \" AND \"\n",
        "    jql += f\"resolution in ({','.join(issue_resolution)})\"\n",
        "\n",
        "  if jql:\n",
        "    jql += \" ORDER BY created DESC\"\n",
        "\n",
        "  return jql\n",
        "\n",
        "jql = create_jql(PROJECT_NAME, ISSUE_TYPES, ISSUE_STATUS, ISSUE_RESOLUTION)\n",
        "all_issues = fetch_issues(JIRA_URL, jql)"
      ],
      "metadata": {
        "id": "hKEBYQ0LnbyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving Commits from GitHub"
      ],
      "metadata": {
        "id": "x8fOPlynw_OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydriller import Repository\n",
        "def fetch_commits(git_url:str, start_date:datetime) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  The method retrieves commits that (1) have been submitted after start_date, \n",
        "  (2) don't belong to a merge request, cherry-pick, or nothing commit, \n",
        "  (3) contain only new lines (not modified or deleted).\n",
        "  \"\"\"\n",
        "\n",
        "  commits = pd.DataFrame(columns=['commit_hash', 'commit_message'])\n",
        "  repository = Repository(path_to_repo = git_url, since = start_date, only_in_branch = GIT_BRANCH_NAME, order = 'reverse')\n",
        "  excluding_pattern = re.compile(\"merge|cherry|nothing\", flags = re.IGNORECASE)\n",
        "\n",
        "  for commit in repository.traverse_commits():\n",
        "\n",
        "    # Eliminating commits that belong to a merge request, cherry-pick, or nothing commit\n",
        "    if re.search(excluding_pattern, commit.msg):\n",
        "      continue\n",
        "\n",
        "    added_lines_count = 0\n",
        "    deleted_lines_count = 0\n",
        "\n",
        "    # Calculating the total number of added lines and deleted lines by traversing through each modified file\n",
        "    for file in commit.modified_files:\n",
        "      added_lines_count += file.added_lines\n",
        "      deleted_lines_count += file.deleted_lines\n",
        "    \n",
        "    # Eliminating commits that have deleted lines or have no added lines\n",
        "    if deleted_lines_count > 0 or added_lines_count == 0:\n",
        "      continue\n",
        "  \n",
        "    new_row = {\n",
        "        'commit_hash' : commit.hash, \n",
        "        'commit_message' : commit.msg \n",
        "        }\n",
        "    commits = commits.append(new_row, ignore_index = True)\n",
        "\n",
        "    print(f\"{commits.shape[0]} commits were added\")\n",
        "\n",
        "  print(\"All commits have been recieved\")\n",
        "  return commits\n",
        "\n",
        "# Retrieving commits that have been submitted after the oldest bug report\n",
        "oldest_bug_report = all_issues.iloc[-1]['issue_created_date']\n",
        "all_commits = fetch_commits(GIT_URL, oldest_bug_report)"
      ],
      "metadata": {
        "id": "5vPKumQcA--7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging BRs with their corresponding BFCs"
      ],
      "metadata": {
        "id": "ukir1FFF2CzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_issues_commits(issues:pd.DataFrame, commits:pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Given all commits and all BRs, this method links each BR to its corresponding BFCs\n",
        "  \"\"\"\n",
        "  merged_issues_commits = pd.DataFrame(columns=['bug_id', 'bug_summery', 'bug_description', 'bfc_hash', 'bfc_message'])\n",
        "\n",
        "  for _, issue in issues.iterrows():\n",
        "    print(f\"Searching for {issue.issue_key}'s commits\")\n",
        "\n",
        "    num = issue.issue_key.split('-')[1]\n",
        "    # Using Regex to find BFCs [1]\n",
        "    patern = re.compile(f\"(?:{PROJECT_BRIEF_NAME}(?:-|_|\\s)*|#){num}(?:\\D|$)\", flags = re.IGNORECASE)\n",
        "    found_commits = commits[commits.commit_message.str.contains(patern, regex = True)]\n",
        "    \n",
        "    for _, commit in found_commits.iterrows():\n",
        "      new_row = {\n",
        "          'bug_id' : issue.issue_key, \n",
        "          'bug_summery' : issue.issue_summary, \n",
        "          'bug_description' : issue.issue_description, \n",
        "          'bfc_hash' : commit.commit_hash, \n",
        "          'bfc_message' : commit.commit_message \n",
        "      }\n",
        "      merged_issues_commits = merged_issues_commits.append(new_row, ignore_index = True)\n",
        "\n",
        "  return merged_issues_commits\n",
        "\n",
        "merged_issues_commits = merge_issues_commits(all_issues, all_commits)"
      ],
      "metadata": {
        "id": "ZPgzCfPUPcbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing the resulted dataset into a CSV file"
      ],
      "metadata": {
        "id": "c3Wn8ZDN3Au2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_issues_commits.to_csv(\"BRs_and_BFCs.csv\")"
      ],
      "metadata": {
        "id": "mg05vI0PLDFP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_issues_commits"
      ],
      "metadata": {
        "id": "OfsYtgbN6Ov2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}